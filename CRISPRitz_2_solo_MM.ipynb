{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import networkx as nx\n",
        "from networkx.classes.function import edges\n",
        "\n",
        "\n",
        "def read_json_data(path):\n",
        "  with open(path) as f:\n",
        "    # Load the contents of the file into a variable\n",
        "    data = f.read()\n",
        "    json_data = json.loads(data)\n",
        "  return json_data\n",
        "\n",
        "\n",
        "def create_positive_graph(json_data):\n",
        "  # Create an empty graph\n",
        "  G = nx.DiGraph()\n",
        "  # Add nodes to the graph\n",
        "  for node in json_data[\"node\"]:\n",
        "    G.add_node(int(node[\"id\"]), seq=node[\"sequence\"])\n",
        "  # Add edges to the graph\n",
        "  for edge in json_data[\"edge\"]:\n",
        "    G.add_edge(int(edge[\"from\"]), int(edge[\"to\"]))\n",
        "  json_data.clear()\n",
        "  return G\n",
        "\n",
        "\n",
        "def reverse_complement(seq):\n",
        "  reverse_compl_seq = \"\"\n",
        "  for c in seq:\n",
        "    if c == 'A':\n",
        "      reverse_compl_seq += 'T'\n",
        "    elif c == 'T':\n",
        "      reverse_compl_seq += 'A'\n",
        "    elif c == 'C':\n",
        "      reverse_compl_seq += 'G'\n",
        "    elif c == 'G':\n",
        "      reverse_compl_seq += 'C'\n",
        "    elif c == 'N':\n",
        "      reverse_compl_seq += 'N'\n",
        "  reverse_compl_seq = reverse_compl_seq[::-1]\n",
        "\n",
        "  return reverse_compl_seq\n",
        "\n",
        "\n",
        "def create_negative_graph(G_positive):\n",
        "  G_negative = G_positive.reverse()\n",
        "  for node in G_negative.nodes():\n",
        "    G_negative.nodes[node][\"seq\"] = reverse_complement(G_negative.nodes[node][\"seq\"])\n",
        "  return G_negative\n",
        "\n",
        "\n",
        "def find_pam_nodes(G, PAM):\n",
        "  gg_nodes = []\n",
        "  gg_splitted_nodes = []\n",
        "  for node in G.nodes():\n",
        "    if PAM[-2:] in G.nodes[node][\"seq\"]:\n",
        "      gg_nodes.append(node)\n",
        "    if G.nodes[node][\"seq\"].startswith(PAM[-1]):\n",
        "      # iterate over neighbors\n",
        "      for neighbor in G.predecessors(node):\n",
        "        if G.nodes[neighbor]['seq'].endswith(PAM[-2]):\n",
        "          gg_splitted_nodes.append(node)\n",
        "\n",
        "  joined_gg_nodes = gg_nodes + list(set(gg_splitted_nodes) - set(gg_nodes))\n",
        "\n",
        "  gg_splitted_nodes.clear()\n",
        "  gg_nodes.clear()\n",
        "  return joined_gg_nodes\n",
        "\n",
        "\n",
        "#function to extract the subgraph contined the PAM.\n",
        "def PAM_DFS(G, PAM_node, PAM, max_seq_depth):\n",
        "  #trash_len = (lenGuida + NBulge) * MAxNumberBulgeConsidered = (20 + 0) * 4\n",
        "  nodes = []\n",
        "  condition = False\n",
        "  trash_len = max_seq_depth - G.nodes[PAM_node][\"seq\"].find(PAM[-2:])\n",
        "  nodes.append(PAM_node)\n",
        "  for node in nodes:\n",
        "    for neighbor in G.predecessors(node):\n",
        "      if trash_len - len(G.nodes[neighbor][\"seq\"]) > 0 or trash_len > 20 :\n",
        "        if neighbor not in nodes:\n",
        "          trash_len -= len(G.nodes[neighbor][\"seq\"])\n",
        "          nodes.append(neighbor)\n",
        "      else:\n",
        "        condition = True\n",
        "        break\n",
        "    if condition:\n",
        "      break\n",
        "\n",
        "  return nodes\n",
        "\n",
        "\n",
        "#function that retunr the edges needed for the function find_all_paths\n",
        "def extract_edges(G, nodes):\n",
        "  edges = []\n",
        "  if len(nodes) > 1:\n",
        "    for n1 in nodes:\n",
        "      for n2 in nodes:\n",
        "        if n1 != n2 and G.has_edge(n1, n2):\n",
        "          edge = (n1, n2)\n",
        "          edges.append(edge)\n",
        "\n",
        "  return edges\n",
        "\n",
        "\n",
        "# Define a function to generate all paths from a list of edges\n",
        "def find_all_paths(edges, target_node):\n",
        "    # Create an empty graph and add the edges to it\n",
        "    graph = nx.DiGraph()\n",
        "    graph.add_edges_from(edges)\n",
        "    # Generate all paths from the graph\n",
        "    all_paths = []\n",
        "    starts_nodes = []\n",
        "    #Da qua faccio il all_simple_path con tutti i nodi che non hanno un arco entrante quindi questi nodi sono quelli\n",
        "    #che vanno nello start_node\n",
        "    for node in graph.nodes():\n",
        "      if graph.in_degree(node) == 0:\n",
        "        starts_nodes.append(node)\n",
        "    for start_node in starts_nodes:\n",
        "      for path in nx.all_simple_paths(graph, source = start_node, target = target_node):\n",
        "        all_paths.append(path)\n",
        "    graph.clear()\n",
        "\n",
        "    return all_paths\n",
        "\n",
        "\n",
        "def paths_on_strand(G, pam_nodes, PAM, max_seq_depth):\n",
        "  all_paths = []\n",
        "  for i in range(len(pam_nodes)):\n",
        "    target_node = pam_nodes[i]\n",
        "    nodes = PAM_DFS(G, target_node, PAM, max_seq_depth)\n",
        "    edges = extract_edges(G, nodes)\n",
        "\n",
        "    if len(nodes) == 1:\n",
        "      all_paths.append([nodes])\n",
        "    else:\n",
        "      all_paths.append(find_all_paths(edges, target_node))\n",
        "\n",
        "  return all_paths\n",
        "\n",
        "\n",
        "def search_PAM_positions(seq, PAM):\n",
        "  positions = []\n",
        "  start = 0\n",
        "\n",
        "  while True:\n",
        "    index = seq.find(PAM[-2:], start)\n",
        "    if index == -1:\n",
        "        break\n",
        "    if index > 20:\n",
        "      positions.append(index - 21)\n",
        "    start = index + 1\n",
        "\n",
        "  return positions\n",
        "\n",
        "\n",
        "def count_mismatch(guide, seq, positions, strand, path, seq_nodes):\n",
        "  max_miss = 4\n",
        "  results = []\n",
        "  for p in positions:\n",
        "    index_guide = 0\n",
        "    count_miss = max_miss\n",
        "    result = []\n",
        "    seq_match = \"\"\n",
        "    while p < p + 20  and index_guide < len(guide): #ex: while p < p + 20  and index_guide < len(guide):\n",
        "      if seq[p] != guide[index_guide]:\n",
        "        seq_match += seq[p].lower()\n",
        "        count_miss -= 1\n",
        "      else:\n",
        "        seq_match += seq[p]\n",
        "      p += 1\n",
        "      index_guide += 1\n",
        "    if count_miss >= 0:\n",
        "      seq_match += seq[p]\n",
        "      seq_match += seq[p + 1]\n",
        "      seq_match += seq[p + 2]\n",
        "      result = {\n",
        "      'path' : path,\n",
        "      'seq_nodes' : seq_nodes,\n",
        "      'seq': seq,\n",
        "      'seq_match': seq_match,\n",
        "      'mismatches': max_miss - count_miss,\n",
        "      'start': p - 20,\n",
        "      'strand': strand\n",
        "      }\n",
        "      #result = filter(result)\n",
        "      results.append(result)\n",
        "  return results\n",
        "\n",
        "\n",
        "def filter(result):\n",
        "  nodes = result['seq_nodes']\n",
        "  result_filtered = None\n",
        "  for i in range(len(nodes)):\n",
        "    if result['seq_match'].upper() in nodes[i]:\n",
        "      result_filtered = {\n",
        "          'path' : [result['path'][i]],\n",
        "          'seq_nodes' : [nodes[i]],\n",
        "          'seq': result['seq'],\n",
        "          'seq_match': result['seq_match'],\n",
        "          'mismatches': result['mismatches'],\n",
        "          'start': result['start'],\n",
        "          'strand': result['strand']\n",
        "      }\n",
        "      break\n",
        "  if result_filtered is not None:\n",
        "    return result_filtered\n",
        "  else:\n",
        "    return result\n",
        "\n",
        "\n",
        "def compare_with_guide(G, all_paths, sgRNA_guide, PAM, strand):\n",
        "  final_results = []\n",
        "  result = []\n",
        "  unique_results = []\n",
        "  guide = sgRNA_guide\n",
        "  for paths in all_paths: #list of paths\n",
        "    for path in paths: #list of nodes\n",
        "      seq = \"\"\n",
        "      seq_nodes = []\n",
        "      for node in path: #each single node in the path\n",
        "        seq += G.nodes[node][\"seq\"]\n",
        "        seq_nodes.append(G.nodes[node][\"seq\"])\n",
        "      positions = search_PAM_positions(seq, PAM)\n",
        "      result = count_mismatch(guide, seq, positions, strand, path, seq_nodes)\n",
        "      if result:\n",
        "        final_results.append(result)\n",
        "  #unique_results = unique_extraction(final_results)\n",
        "  return unique_results\n",
        "\n",
        "\n",
        "def unique_extraction(final_results):\n",
        "  unique_list = []\n",
        "  for i in range(len(final_results)):\n",
        "      path = final_results[i][0]['path']\n",
        "      if path not in [item[0]['path'] for item in unique_list]:\n",
        "          unique_list.append(final_results[i])\n",
        "  return unique_list\n"
      ],
      "metadata": {
        "id": "udEDXlrjYnMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2G6BBq1YcLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124e31fe-c89b-46f9-f5fa-5e5152306997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'path': [3200439, 3200440], 'seq_nodes': ['G', 'AGTCCGTGTAGAAGCAGAGGGGCTGTACAGCT'], 'seq': 'GAGTCCGTGTAGAAGCAGAGGGGCTGTACAGCT', 'seq_match': 'GAGTCCGtGtAGAAGcAGAgGGG', 'mismatches': 4, 'start': 0, 'strand': '+'}]\n",
            "[{'path': [1094870, 1094872, 1094873], 'seq_nodes': ['GAGTCCCAGCAGAAGGA', 'C', 'GAGGGGCTGTGA'], 'seq': 'GAGTCCCAGCAGAAGGACGAGGGGCTGTGA', 'seq_match': 'GAGTCCcAGCAGAAGgAcgAGGG', 'mismatches': 4, 'start': 0, 'strand': '+'}]\n",
            "[{'path': [1094870, 1094871, 1094873], 'seq_nodes': ['GAGTCCCAGCAGAAGGA', 'G', 'GAGGGGCTGTGA'], 'seq': 'GAGTCCCAGCAGAAGGAGGAGGGGCTGTGA', 'seq_match': 'GAGTCCcAGCAGAAGgAGgAGGG', 'mismatches': 3, 'start': 0, 'strand': '+'}]\n",
            "[{'path': [1094870, 3760415, 1094873], 'seq_nodes': ['GAGTCCCAGCAGAAGGA', 'G', 'GAGGGGCTGTGA'], 'seq': 'GAGTCCCAGCAGAAGGAGGAGGGGCTGTGA', 'seq_match': 'GAGTCCcAGCAGAAGgAGgAGGG', 'mismatches': 3, 'start': 0, 'strand': '+'}]\n",
            "[{'path': [1041825, 1041826], 'seq_nodes': ['CAGGGCTGGGTGACACCTGGGAGAAGAGTCCA', 'GGCAGAAGGAGCAGGGAAGGTGAGGTCCTTGG'], 'seq': 'CAGGGCTGGGTGACACCTGGGAGAAGAGTCCAGGCAGAAGGAGCAGGGAAGGTGAGGTCCTTGG', 'seq_match': 'GAGTCCagGCAGAAGgAGcAGGG', 'mismatches': 4, 'start': 25, 'strand': '+'}]\n",
            "[{'path': [2288516], 'seq_nodes': ['GTGTGACAGAGCAAAAGAAGAAAGGTCAGTAG'], 'seq': 'GTGTGACAGAGCAAAAGAAGAAAGGTCAGTAG', 'seq_match': 'GtGaCaGAGCAaAAGAAGAAAGG', 'mismatches': 4, 'start': 2, 'strand': '-'}]\n",
            "[{'path': [3938907, 3938905, 3938904], 'seq_nodes': ['CGTGAGAGTGGGAAGAGAAGAAG', 'A', 'ATGGCCATGGAGGGAGG'], 'seq': 'CGTGAGAGTGGGAAGAGAAGAAGAATGGCCATGGAGGGAGG', 'seq_match': 'GAGTggGAagAGAAGAAGAATGG', 'mismatches': 4, 'start': 5, 'strand': '-'}]\n",
            "[{'path': [3869093, 3869092, 3869090], 'seq_nodes': ['AAGGATGGAGGGCAGGAGCCAGGGCCTGAGTC', 'C', 'GAGCAGGAGAGGCCAGGCTCACTTTAGA'], 'seq': 'AAGGATGGAGGGCAGGAGCCAGGGCCTGAGTCCGAGCAGGAGAGGCCAGGCTCACTTTAGA', 'seq_match': 'GAGTCCGAGCAGgAGAgGccAGG', 'mismatches': 4, 'start': 27, 'strand': '-'}]\n",
            "[{'path': [892606, 892605, 892603], 'seq_nodes': ['GAGTGCACCAGACCCAGCCTTGTAGTT', 'C', 'GAGGAGAGGAAGAAAGGC'], 'seq': 'GAGTGCACCAGACCCAGCCTTGTAGTTCGAGGAGAGGAAGAAAGGC', 'seq_match': 'tAGTtCGAGgAGAgGAAGAAAGG', 'mismatches': 4, 'start': 22, 'strand': '-'}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import networkx as nx\n",
        "\n",
        "json_data = read_json_data('/content/drive/MyDrive/chr_22.json')\n",
        "G_positive = create_positive_graph(json_data)\n",
        "G_positive.add_edge(3760415,1094873)\n",
        "G_positive.add_edge(1094870,3760415)\n",
        "G_negative = create_negative_graph(G_positive)\n",
        "PAM = \"NGG\"\n",
        "\n",
        "positive_pam_nodes = find_pam_nodes(G_positive, PAM)\n",
        "all_positive_paths = paths_on_strand(G_positive, positive_pam_nodes, PAM, 30)\n",
        "positive_strand_results = compare_with_guide(G_positive, all_positive_paths, \"GAGTCCGAGCAGAAGAAGAA\", PAM, '+')\n",
        "\n",
        "for result in positive_strand_results:\n",
        "  print(result)\n",
        "\n",
        "negative_pam_nodes = find_pam_nodes(G_negative, PAM)\n",
        "all_negative_paths = paths_on_strand(G_negative, negative_pam_nodes, PAM, 30)\n",
        "negative_strand_results = compare_with_guide(G_negative, all_negative_paths, \"GAGTCCGAGCAGAAGAAGAA\", PAM, '-')\n",
        "\n",
        "for result in negative_strand_results:\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FXqxr2P7KZ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}